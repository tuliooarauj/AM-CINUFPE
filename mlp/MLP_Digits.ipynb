{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ayi00KQngCv"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------\n",
        "# 0. Pacotes\n",
        "# --------------------------------------------------\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import (train_test_split,\n",
        "                                     StratifiedKFold,\n",
        "                                     cross_validate)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             f1_score,\n",
        "                             make_scorer)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. Wrapper: MLP com escolha de inicialização\n",
        "# --------------------------------------------------\n",
        "class InitMLP(MLPClassifier):\n",
        "    \"\"\"\n",
        "    MLPClassifier com escolha da estratégia de inicialização:\n",
        "    'glorot' (padrão), 'normal' ou 'he_uniform'.\n",
        "    \"\"\"\n",
        "    def __init__(self, *,              # força kwargs-only\n",
        "                 weight_init=\"glorot\",  # novo parâmetro\n",
        "                 **kwargs):             # passa o resto para o MLP original\n",
        "        super().__init__(**kwargs)\n",
        "        self.weight_init = weight_init\n",
        "\n",
        "    # --- substitui os pesos depois da _initialize do pai ---\n",
        "    def _initialize(self, y, layer_units, dtype):\n",
        "        super()._initialize(y, layer_units, dtype)\n",
        "        rng = self._random_state\n",
        "        for i, (fan_in, fan_out) in enumerate(zip(layer_units[:-1],\n",
        "                                                  layer_units[1:])):\n",
        "            shape = (fan_in, fan_out)\n",
        "            if self.weight_init == \"normal\":\n",
        "                scale = 1. / np.sqrt(fan_in)\n",
        "                self.coefs_[i] = rng.normal(0.0, scale, size=shape)\n",
        "            elif self.weight_init == \"he_uniform\":\n",
        "                limit = np.sqrt(6. / fan_in)\n",
        "                self.coefs_[i] = rng.uniform(-limit, limit, size=shape)\n",
        "            # ‘glorot’ já foi gerado pelo método do pai\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. Dados\n",
        "# --------------------------------------------------\n",
        "X, y = load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=42)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. Configurações (arquitetura, L2, inicialização)\n",
        "# --------------------------------------------------\n",
        "configs = {\n",
        "    #  nome        layers                   alpha       init\n",
        "    \"glo64_l2-4\": dict(layers=(64,),       alpha=1e-4, weight_init=\"glorot\"),\n",
        "    \"glo64_l2-3\": dict(layers=(64,),       alpha=1e-3, weight_init=\"glorot\"),\n",
        "    \"norm64_l2-4\":dict(layers=(64,),       alpha=1e-4, weight_init=\"normal\"),\n",
        "    \"heDeep_l2-4\":dict(layers=(128, 64),   alpha=1e-4, weight_init=\"he_uniform\"),\n",
        "    \"heDeep_l2-3\":dict(layers=(128, 64),   alpha=1e-3, weight_init=\"he_uniform\"),\n",
        "    ## ... adicione mais 5 ou mais combinacoes ...\n",
        "    # \"reluWide_l2-2\": dict(layers=(n,n),\n",
        "    #                   alpha=1e-2,\n",
        "    #                   weight_init=\"he_uniform\",\n",
        "    #                   activation=\"relu\")\n",
        "}\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. Validação cruzada no treino\n",
        "# --------------------------------------------------\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scoring = {\"acc\": \"accuracy\",\n",
        "           \"f1\":  make_scorer(f1_score, average=\"macro\")}\n",
        "\n",
        "rows = []\n",
        "for name, p in configs.items():\n",
        "    clf = InitMLP(\n",
        "        hidden_layer_sizes=p[\"layers\"],\n",
        "        alpha=p[\"alpha\"],\n",
        "        weight_init=p[\"weight_init\"],\n",
        "        max_iter=200,\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=5,\n",
        "        learning_rate_init=1e-3,\n",
        "        solver=\"adam\",\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "\n",
        "    pipe = Pipeline([(\"scale\", StandardScaler()),\n",
        "                     (\"clf\",   clf)])\n",
        "\n",
        "    res = cross_validate(pipe, X_train, y_train,\n",
        "                         cv=cv, scoring=scoring,\n",
        "                         return_train_score=False)\n",
        "\n",
        "    rows.append({\n",
        "        \"config\":   name,\n",
        "        \"layers\":   p[\"layers\"],\n",
        "        \"alpha\":    p[\"alpha\"],\n",
        "        \"init\":     p[\"weight_init\"],\n",
        "        \"f1_mean\":  res[\"test_f1\"].mean(),\n",
        "        \"f1_std\":   res[\"test_f1\"].std(),\n",
        "        \"acc_mean\": res[\"test_acc\"].mean(),\n",
        "        \"acc_std\":  res[\"test_acc\"].std(),\n",
        "    })\n",
        "\n",
        "    print(f\"{name:12s} | CV macro-F1 = \"\n",
        "          f\"{res['test_f1'].mean():.3f} ± {res['test_f1'].std():.3f}\")\n",
        "\n",
        "summary = (pd.DataFrame(rows)\n",
        "              .sort_values(\"f1_mean\", ascending=False))\n",
        "\n",
        "best_conf  = summary.iloc[0]\n",
        "best_name  = best_conf[\"config\"]\n",
        "best_param = configs[best_name]\n",
        "print(\"\\n>> Selecionado:\", best_name, dict(best_param))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5. Re-treino em todo o treino + teste final\n",
        "# --------------------------------------------------\n",
        "best_clf = InitMLP(\n",
        "    hidden_layer_sizes=best_param[\"layers\"],\n",
        "    alpha=best_param[\"alpha\"],\n",
        "    weight_init=best_param[\"weight_init\"],\n",
        "    max_iter=200,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=5,\n",
        "    learning_rate_init=1e-3,\n",
        "    solver=\"adam\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "best_pipe = Pipeline([(\"scale\", StandardScaler()),\n",
        "                      (\"clf\",   best_clf)])\n",
        "best_pipe.fit(X_train, y_train)\n",
        "\n",
        "y_pred   = best_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "test_f1  = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"\\n>> TESTE | acc = {test_acc:.3f} | macro-F1 = {test_f1:.3f}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 6. Tabela resumo (para o relatório)\n",
        "# --------------------------------------------------\n",
        "print(\"\\nResumo completo:\")\n",
        "display(summary[[\"config\", \"layers\", \"alpha\", \"init\",\n",
        "                 \"acc_mean\", \"acc_std\", \"f1_mean\", \"f1_std\"]])\n"
      ]
    }
  ]
}